usage: train.py [-h] [--no-progress-bar] [--log-interval N]
                [--log-format {json,none,simple,tqdm}] [--seed N] [--fp16]
                [--task TASK] [--skip-invalid-size-inputs-valid-test]
                [--max-tokens N] [--max-sentences N] [--train-subset SPLIT]
                [--valid-subset SPLIT] [--max-sentences-valid N]
                [--distributed-world-size N]
                [--distributed-rank DISTRIBUTED_RANK]
                [--distributed-backend DISTRIBUTED_BACKEND]
                [--distributed-init-method DISTRIBUTED_INIT_METHOD]
                [--distributed-port DISTRIBUTED_PORT] [--device-id DEVICE_ID]
                --arch ARCH [--criterion CRIT] [--max-epoch N]
                [--max-update N] [--clip-norm NORM] [--sentence-avg]
                [--update-freq N] [--optimizer OPT] [--lr LR_1,LR_2,...,LR_N]
                [--momentum M] [--weight-decay WD]
                [--lr-scheduler LR_SCHEDULER] [--lr-shrink LS] [--min-lr LR]
                [--min-loss-scale D] [--save-dir DIR]
                [--restore-file RESTORE_FILE] [--save-interval N]
                [--save-interval-updates N] [--keep-interval-updates N]
                [--no-save] [--no-epoch-checkpoints] [--validate-interval N]

Facebook AI Research Sequence-to-Sequence Toolkit -- Trainer

optional arguments:
  -h, --help            show this help message and exit
  --no-progress-bar     disable progress bar
  --log-interval N      log progress every N batches (when progress bar is
                        disabled)
  --log-format {json,none,simple,tqdm}
                        log format to use
  --seed N              pseudo random number generator seed
  --fp16                use FP16
  --task TASK           task: translation, language_modeling (default:
                        translation)

Dataset and data loading:
  --skip-invalid-size-inputs-valid-test
                        ignore too long or too short lines in valid and test
                        set
  --max-tokens N        maximum number of tokens in a batch
  --max-sentences N, --batch-size N
                        maximum number of sentences in a batch
  --train-subset SPLIT  data subset to use for training (train, valid, test)
  --valid-subset SPLIT  comma separated list of data subsets to use for
                        validation (train, valid, valid1, test, test1)
  --max-sentences-valid N
                        maximum number of sentences in a validation batch
                        (defaults to --max-sentences)

Distributed training:
  --distributed-world-size N
                        total number of GPUs across all nodes (default: all
                        visible GPUs)
  --distributed-rank DISTRIBUTED_RANK
                        rank of the current worker
  --distributed-backend DISTRIBUTED_BACKEND
                        distributed backend
  --distributed-init-method DISTRIBUTED_INIT_METHOD
                        typically tcp://hostname:port that will be used to
                        establish initial connetion
  --distributed-port DISTRIBUTED_PORT
                        port number (not required if using --distributed-init-
                        method)
  --device-id DEVICE_ID
                        which GPU to use (usually configured automatically)

Model configuration:
  --arch ARCH, -a ARCH  model architecture: transformer_lm,
                        transformer_lm_big, transformer_lm_wiki103,
                        transformer_lm_gbw, transformer,
                        transformer_iwslt_de_en, transformer_wmt_en_de,
                        transformer_vaswani_wmt_en_de_big,
                        transformer_vaswani_wmt_en_fr_big,
                        transformer_wmt_en_de_big,
                        transformer_wmt_en_de_big_t2t, lstm,
                        lstm_wiseman_iwslt_de_en, lstm_luong_wmt_en_de,
                        fconv_self_att, fconv_self_att_wp, fconv_lm,
                        fconv_lm_dauphin_wikitext103, fconv_lm_dauphin_gbw,
                        fconv, fconv_iwslt_de_en, fconv_wmt_en_ro,
                        fconv_wmt_en_de, fconv_wmt_en_fr (default: fconv)
  --criterion CRIT      training criterion: label_smoothed_cross_entropy,
                        adaptive_loss, cross_entropy (default: cross_entropy)

Optimization:
  --max-epoch N, --me N
                        force stop training at specified epoch
  --max-update N, --mu N
                        force stop training at specified update
  --clip-norm NORM      clip threshold of gradients
  --sentence-avg        normalize gradients by the number of sentences in a
                        batch (default is to normalize by number of tokens)
  --update-freq N       update parameters every N_i batches, when in epoch i
  --optimizer OPT       optimizer: adagrad, sgd, nag, adam (default: nag)
  --lr LR_1,LR_2,...,LR_N, --learning-rate LR_1,LR_2,...,LR_N
                        learning rate for the first N epochs; all epochs >N
                        using LR_N (note: this may be interpreted differently
                        depending on --lr-scheduler)
  --momentum M          momentum factor
  --weight-decay WD, --wd WD
                        weight decay
  --lr-scheduler LR_SCHEDULER
                        learning rate scheduler: inverse_sqrt,
                        reduce_lr_on_plateau, fixed (default:
                        reduce_lr_on_plateau)
  --lr-shrink LS        learning rate shrink factor for annealing, lr_new =
                        (lr * lr_shrink)
  --min-lr LR           minimum learning rate
  --min-loss-scale D    minimum loss scale (for FP16 training)

Checkpointing:
  --save-dir DIR        path to save checkpoints
  --restore-file RESTORE_FILE
                        filename in save-dir from which to load checkpoint
  --save-interval N     save a checkpoint every N epochs
  --save-interval-updates N
                        save a checkpoint (and validate) every N updates
  --keep-interval-updates N
                        keep last N checkpoints saved with --save-interval-
                        updates
  --no-save             don't save models or checkpoints
  --no-epoch-checkpoints
                        only store last and best checkpoints
  --validate-interval N
                        validate every N epochs
